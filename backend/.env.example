# ============================================
# Interview Assistant - Environment Variables
# Location: backend/.env.example
# ============================================
# Copy to .env and fill in your values:
#   cp .env.example .env
# ============================================

# ===========================================
# Application Settings
# ===========================================
APP_ENV=development          # development | production
DEBUG=true                   # Set to false in production
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO               # DEBUG | INFO | WARNING | ERROR

# CORS - Comma separated origins (update for production)
CORS_ORIGINS=http://localhost:5173,http://localhost:3000

# ===========================================
# Database (Supabase Free Tier)
# ===========================================
# Get credentials from: https://supabase.com/dashboard
DATABASE_URL=postgresql://user:password@localhost:5432/interview_assistant
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-supabase-anon-key

# ===========================================
# Local LLM (Ollama)
# ===========================================
# For local development:
OLLAMA_HOST=http://localhost:11434
# For Docker Compose (uncomment):
# OLLAMA_HOST=http://host.docker.internal:11434

# Model selection based on instance size:
# - t2.micro (1GB RAM):  tinyllama, phi
# - t2.small (2GB RAM):  llama3.2:1b, gemma:2b
# - t2.medium+ (4GB+):   llama3.2, mistral, deepseek-coder
OLLAMA_MODEL=tinyllama

# Embedding model (for RAG - optional)
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# ===========================================
# Whisper (Speech Recognition)
# ===========================================
# Model sizes and approximate memory:
# - tiny:   ~75MB  (fastest, less accurate)
# - base:   ~150MB (good balance for t2.micro)
# - small:  ~500MB (better accuracy)
# - medium: ~1.5GB (high accuracy)
# - large:  ~3GB   (best accuracy, needs GPU)
WHISPER_MODEL_SIZE=base
WHISPER_DEVICE=cpu           # cpu | cuda

# ===========================================
# Judge0 (Code Execution)
# ===========================================
# Local Docker instance (recommended for free tier):
JUDGE0_USE_HOSTED=false
JUDGE0_BASE_URL=http://localhost:2358

# OR use hosted RapidAPI (has rate limits):
# JUDGE0_USE_HOSTED=true
# JUDGE0_BASE_URL=https://judge0-ce.p.rapidapi.com
# JUDGE0_API_KEY=your_rapidapi_key_here

# Execution limits
JUDGE0_TIMEOUT=5             # seconds per execution
JUDGE0_MEMORY_LIMIT=128000   # KB (128MB)

# ===========================================
# Vision / Image Analysis
# ===========================================
# For system design diagram analysis
# Option 1: Hugging Face API (free tier available)
# Get token from: https://huggingface.co/settings/tokens
# Token needs "Make calls to Inference Providers" permission
HF_TOKEN=your_hugging_face_token_here
VISION_MODEL=Salesforce/blip-image-captioning-large

# Option 2: Local Ollama vision model (needs more RAM)
# VISION_USE_OLLAMA=true
# VISION_OLLAMA_MODEL=llava

# ===========================================
# AWS Settings
# ===========================================
AWS_REGION=us-east-1
# Note: On EC2, use IAM role instead of access keys
# AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are auto-loaded from IAM role

# S3 bucket for session artifacts (created by CDK)
S3_BUCKET_NAME=interview-assistant-artifacts

# ===========================================
# Feature Flags
# ===========================================
ENABLE_HALLUCINATION_CHECK=true
ENABLE_BODY_LANGUAGE=true      # MediaPipe runs in frontend
ENABLE_CODE_EXECUTION=true
ENABLE_SPEECH_ANALYSIS=true
ENABLE_VISION_ANALYSIS=true

# ===========================================
# Session & Security
# ===========================================
# Session expiry (seconds)
SESSION_EXPIRY=3600            # 1 hour

# Rate limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=60           # seconds

# ===========================================
# EC2 Deployment Specific
# ===========================================
# These are auto-set by systemd service on EC2
# Uncomment for local Docker testing:
# OLLAMA_HOST=http://host.docker.internal:11434
# JUDGE0_BASE_URL=http://judge0:2358